<div align="center">
  <img src="https://raw.githubusercontent.com/ixchio/n0x/main/public/icon.png" width="80" alt="n0x logo" style="border-radius:20%" />
</div>

<h1 align="center">n0x</h1>

<div align="center">
  <strong>The full AI stack in one browser tab.</strong><br />
  LLM inference, autonomous agents, RAG, code execution, image generation â€” zero backend, zero API keys.
</div>

<br />

<div align="center">
  <a href="https://n0x.vercel.app">Live Demo</a>
  <span> Â· </span>
  <a href="#architecture">Architecture</a>
  <span> Â· </span>
  <a href="#why-this-exists">Thesis</a>
  <span> Â· </span>
  <a href="#quick-start">Quick Start</a>
</div>

<br />

## Why This Exists

Cloud AI is a moat for vendors, not users. Every API call ships your data to someone else's GPU, adds latency, and costs money. n0x proves that a complete AI workflow â€” inference, tool use, document retrieval, code execution â€” can run **entirely inside a browser tab** using WebGPU, WASM, and IndexedDB. No server. No account. No data leaves your machine.

## Core Systems

### WebGPU Inference Engine
Direct-to-metal LLM execution via MLC/WebLLM. Quantized models (q4f16) hit **35â€“50 tokens/sec** on consumer hardware. 16 models available across 5 tiers â€” downloaded once, cached in browser storage permanently. Real-time TPS telemetry displayed by default.

### Autonomous Agent Loop (ReAct)
A full ReAct-style reasoning engine running in-browser. The LLM autonomously chains tool calls â€” web search, document retrieval, Python execution, memory â€” across multiple iterations until it solves the problem. Features:
- Multi-strategy JSON parsing (handles malformed LLM output)
- Per-tool execution timeouts with AbortController cancellation
- Context window budgeting (prevents OOM on small model windows)
- Loop detection (catches repeated tool calls)
- Live trace UI with per-step timing

### WASM Vector Search (RAG)
Drag-and-drop document ingestion (PDF, TXT, MD, JSON). Text is chunked, embedded via `all-MiniLM-L6-v2` (transformers.js), and indexed for cosine similarity search using `voy-search`. Entire pipeline runs in a Web Worker â€” zero UI blocking. Vectors are cached in IndexedDB across sessions.

### Sandboxed Python Runtime
Client-side Python execution via Pyodide (WASM). Code output feeds back into LLM context. Self-healing: if execution fails, the error is automatically sent back to the LLM for a retry.

### Additional Capabilities
- **Deep Search**: DuckDuckGo + Wikipedia synthesis via lightweight serverless proxy
- **Image Generation**: Pollinations AI (Flux) with Stable Horde fallback
- **Persistent Memory**: Long-term conversational memory via IndexedDB
- **Voice I/O**: Browser-native speech-to-text and text-to-speech
- **5 Persona Modes**: Default, Senior Engineer, Writer, Tutor, Analyst â€” each with detailed response formatting rules

## Architecture

```text
[User Input] â†’ [Mode Router]
                   â”‚
                   â”œâ”€â†’ [Agent Mode] â”€â”€â†’ ReAct Loop (Thought â†’ Action â†’ Observation)
                   â”‚                         â”‚
                   â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚         â–¼               â–¼               â–¼
                   â”‚    [Web Search]   [RAG Search]    [Python Exec]
                   â”‚         â”‚               â”‚               â”‚
                   â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚                         â–¼
                   â”‚                  [Next Iteration or Final Answer]
                   â”‚
                   â”œâ”€â†’ [Direct Mode] â”€â”€â†’ Context Assembly
                   â”‚         â”‚
                   â”‚         â”œâ”€â”€ Search results (if enabled)
                   â”‚         â”œâ”€â”€ Document context (if attached)
                   â”‚         â””â”€â”€ Memory context (if enabled)
                   â”‚         â”‚
                   â”‚         â–¼
                   â”‚    [WebGPU LLM] â†’ Streaming Response
                   â”‚
                   â””â”€â†’ [Image Mode] â”€â”€â†’ Pollinations / Stable Horde API
```

## Models & Memory Footprint

All weights are quantized (q4f16) for optimal VRAM usage. Default: **Qwen 2.5 1.5B**.

| Tier | Model | VRAM | Tokens/Sec |
|---|---|---|---|
| âš¡ Fast | SmolLM2 360M, Qwen 0.5B, TinyLlama 1.1B | 250MBâ€“600MB | 50+ |
| âš–ï¸ Balanced | **Qwen 2.5 1.5B** *(default)*, Llama 3.2 1B, Phi-3.5 Mini | 700MBâ€“2GB | 30â€“40 |
| ğŸš€ Powerful | Llama 3.2 3B, Qwen 3B, Mistral 7B, Hermes 2 Pro 8B | 2â€“4.5GB | 15â€“25 |
| ğŸ’» Code | Qwen Coder 1.5B, Qwen Coder 7B, DeepSeek Coder 1.3B | 800MBâ€“4GB | 25â€“40 |

## Quick Start

Requires Node 18+ and Chromium 113+ (WebGPU).

```bash
git clone https://github.com/ixchio/n0x.git
cd n0x
npm install
npm run dev
```

Open `localhost:3000`. Default model (~1GB) downloads on first load and is cached permanently.

## Privacy Model

The inference graph and orchestration layer run entirely in the browser. PII and proprietary content never transit a network boundary. Two optional external hooks exist, both toggleable:
- **Search**: Serverless proxy for DuckDuckGo/Tavily (CORS bypass)
- **Image Gen**: Pollinations AI / Stable Horde API

Disabling both guarantees a **100% air-gapped** runtime.

## Stack

`Next.js 14` Â· `TypeScript` Â· `WebLLM (WebGPU)` Â· `Pyodide (WASM)` Â· `Transformers.js` Â· `Voy Search` Â· `Tailwind CSS` Â· `Framer Motion` Â· `Zustand`

## License

MIT Â© [ixchio](https://github.com/ixchio)
